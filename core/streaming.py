# core/streaming.py

import threading
import queue
import time
from typing import Tuple, Callable, Optional

import numpy as np

from core.config import AppConfig
from audio.capture import resolve_device_index, record_once_with_index
from stt.engine import create_stt_engine
from core.debug_config import DEBUG, DEBUG_VAD, DEBUG_STT, DEBUG_CAPTURE
from core.translate import translate_text


AudioChunk = Tuple[np.ndarray, int]
CaptionCallback = Callable[[str, Optional[str]], None]

def process_caption_text(
    stt_text: str,
    app_cfg: AppConfig,
) -> tuple[str, str]:
    """
    STT ê²°ê³¼ë¥¼ ë°›ì•„ì„œ:
    - original_text: STT ì›ë¬¸
    - caption_text: ìë§‰ì— í‘œì‹œí•  í…ìŠ¤íŠ¸ (ë²ˆì—­ ì ìš©)
    """
    original_text = stt_text.strip()
    if not original_text:
        return "", ""

    speech_lang = app_cfg.stt.speech_language
    caption_lang = app_cfg.stt.caption_language

    if caption_lang == speech_lang:
        return original_text, original_text

    translated = translate_text(
        text=original_text,
        src=speech_lang,
        tgt=caption_lang,
    )

    return original_text, translated

# ------------------- 1) ë””ì½” ëŒ€í™”ìš© (VAD + endpoint) ------------------- #
def run_stream_pipeline_vad(app_cfg: AppConfig, caption_callback=None):

    from core.debug_config import DEBUG, DEBUG_VAD, DEBUG_STT, DEBUG_CAPTURE

    audio_cfg = app_cfg.audio
    stt_cfg = app_cfg.stt

    frame_sec = audio_cfg.chunk_duration_sec  # 0.5ì´ˆ

    # VAD íŒŒë¼ë¯¸í„°
    VOICE_ENERGY_TH   = 0.005
    SILENCE_ENERGY_TH = 0.004
    MIN_UTTER_SEC     = 0.8
    MAX_UTTER_SEC     = 15.0
    END_SILENCE_SEC   = 0.40

    print("=== STREAM (VAD + endpoint, ë””ì½” ëŒ€í™”ìš©) ===")

    device_index = resolve_device_index(audio_cfg)
    stt_engine = create_stt_engine(stt_cfg)

    audio_queue: "queue.Queue[AudioChunk]" = queue.Queue(maxsize=20)
    stop_flag = threading.Event()

    current_frames = []
    current_dur = 0.0
    silence_dur = 0.0
    in_speech = False

    # ---------------- capture thread ---------------- #
    def capture_worker():
        while not stop_flag.is_set():
            audio_data, sr = record_once_with_index(audio_cfg, device_index)

            if DEBUG_CAPTURE:
                print(f"[CAP] frame captured ({frame_sec}s), samples={len(audio_data)}")

            try:
                audio_queue.put((audio_data, sr), timeout=1.0)
            except queue.Full:
                print("[WARN] audio queue full, dropping frame")

    # ---------------- finalize utterance ---------------- #
    def finalize_utterance():
        nonlocal current_frames, current_dur

        if not current_frames or current_dur < MIN_UTTER_SEC:
            current_frames = []
            current_dur = 0.0
            return

        audio = np.concatenate(current_frames, axis=0)
        sr = audio_cfg.sample_rate

        if DEBUG_STT:
            print(f"[STT] START: duration={current_dur:.2f}s")

        stt_text = stt_engine.transcribe(audio, sr).strip()

        if DEBUG_STT:
            print(f"[STT] RAW: '{stt_text}'")

        original, caption = process_caption_text(stt_text, app_cfg)

        if not original:
            current_frames = []
            current_dur = 0.0
            return

        if caption_callback:
            caption_callback(caption, original)
        else:
            print(">>>", caption)

        current_frames = []
        current_dur = 0.0

    # ---------------- stt worker ---------------- #
    def stt_worker():
        nonlocal current_frames, current_dur, silence_dur, in_speech

        while not stop_flag.is_set():
            try:
                frame, sr = audio_queue.get(timeout=1.0)
            except queue.Empty:
                continue

            energy = float(np.mean(np.abs(frame)))

            if DEBUG_VAD:
                print(f"[VAD] energy={energy:.5f}")

            if energy > VOICE_ENERGY_TH:
                current_frames.append(frame)
                current_dur += frame_sec
                silence_dur = 0.0
                in_speech = True

                if current_dur >= MAX_UTTER_SEC:
                    finalize_utterance()
                    in_speech = False
                    silence_dur = 0.0

            else:
                if in_speech:
                    silence_dur += frame_sec
                    if silence_dur >= END_SILENCE_SEC:
                        finalize_utterance()
                        in_speech = False
                        silence_dur = 0.0

    t1 = threading.Thread(target=capture_worker, daemon=True)
    t2 = threading.Thread(target=stt_worker, daemon=True)
    t1.start()
    t2.start()

    try:
        while True:
            time.sleep(0.05)   # ğŸŒŸ ë ‰ ë°©ì§€
    except KeyboardInterrupt:
        stop_flag.set()
        t1.join(timeout=2.0)
        t2.join(timeout=2.0)


# ------------------- 2) ë¸Œê¸ˆ/ì˜ìƒìš© (ê³ ì • ê¸¸ì´ ì²­í¬) ------------------- #
def run_stream_pipeline_fixed(
    app_cfg: AppConfig,
    caption_callback: Optional[CaptionCallback] = None,
) -> None:
    """
    ë¸Œê¸ˆ/ì˜ìƒ ëª¨ë“œ (ê³ ì • ì²­í¬ + íŒŒì´í”„ë¼ì¸):

    - audio_cfg.chunk_duration_sec (ì˜ˆ: 7ì´ˆ) ê¸¸ì´ë¡œ ê³ ì • ë…¹ìŒ
    - ìº¡ì²˜ ìŠ¤ë ˆë“œ: 7ì´ˆì”© ê³„ì† ë…¹ìŒí•´ì„œ íì— ë„£ê¸°
    - STT ìŠ¤ë ˆë“œ: íì—ì„œ êº¼ë‚´ì„œ STT â†’ ìë§‰ ì¶œë ¥
    - ë…¹ìŒê³¼ STTë¥¼ ê²¹ì³ì„œ ëŒë ¤, ì²´ê° ë”œë ˆì´ë¥¼ ì¤„ì¸ë‹¤.
    """
    audio_cfg = app_cfg.audio
    stt_cfg = app_cfg.stt

    chunk_sec = audio_cfg.chunk_duration_sec

    print("=== discord_capcap :: STREAM (ê³ ì • ì²­í¬, ë¸Œê¸ˆ/ì˜ìƒìš© íŒŒì´í”„ë¼ì¸) ===")
    print(f"- device: {audio_cfg.device_name}")
    print(f"- chunk: {chunk_sec} sec")
    print(f"- model: {stt_cfg.model_name}, device={stt_cfg.device}, lang={stt_cfg.language}")
    print()

    device_index = resolve_device_index(audio_cfg)
    stt_engine = create_stt_engine(stt_cfg)

    audio_queue: "queue.Queue[AudioChunk]" = queue.Queue(maxsize=3)
    stop_flag = threading.Event()

    # ---------------- ìº¡ì²˜ ìŠ¤ë ˆë“œ ---------------- #
    def capture_worker():
        while not stop_flag.is_set():
            start_t = time.time()
            audio_data, sr = record_once_with_index(audio_cfg, device_index)
            rec_dur = time.time() - start_t
            print(f"[*] ìº¡ì²˜ ì™„ë£Œ: {rec_dur:.2f}s, samples={len(audio_data)}")

            try:
                audio_queue.put((audio_data, sr), timeout=1.0)
            except queue.Full:
                print("[WARN] fixed-mode audio_queue full, dropping chunk")

    # ---------------- STT ìŠ¤ë ˆë“œ ---------------- #
    def stt_worker():
        while not stop_flag.is_set():
            try:
                audio_data, sr = audio_queue.get(timeout=1.0)
            except queue.Empty:
                continue

            stt_start = time.time()
            if DEBUG_STT:
                print(f"[*] STT í˜¸ì¶œ(FIXED): durationâ‰ˆ{chunk_sec:.2f}s, samples={len(audio_data)}")

            stt_text = stt_engine.transcribe(audio_data, sr).strip()

            original, caption = process_caption_text(stt_text, app_cfg)

            if not original:
                if caption_callback:
                    caption_callback("...", None)
                continue

            if caption_callback:
                caption_callback(caption, original)
            else:
                print(f">>> [CAPTION] {caption}")

    t_cap = threading.Thread(target=capture_worker, daemon=True)
    t_stt = threading.Thread(target=stt_worker, daemon=True)
    t_cap.start()
    t_stt.start()

    try:
        while True:
            time.sleep(0.1)
    except KeyboardInterrupt:
        stop_flag.set()
        t_cap.join(timeout=2.0)
        t_stt.join(timeout=2.0)
        print("[*] ë¸Œê¸ˆ/ì˜ìƒ ëª¨ë“œ ì¢…ë£Œ")

# ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„ ìœ„í•´ ê¸°ë³¸ run_stream_pipelineì€ VAD ë²„ì „ìœ¼ë¡œ ë§¤í•‘
def run_stream_pipeline(
    app_cfg: AppConfig,
    caption_callback: Optional[CaptionCallback] = None,
) -> None:
    return run_stream_pipeline_vad(app_cfg, caption_callback)
